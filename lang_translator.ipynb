{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6190837b-b43a-4988-96c3-8be4af6985f4",
   "metadata": {},
   "source": [
    "# Section 1: Problem Definition & Objective\n",
    "\n",
    "## a. Selected Project Track\n",
    "The selected project track for this work is Artificial Intelligence Applications, with a focus on Natural Language Processing (NLP). The project specifically addresses the task of Neural Machine Translation, where deep learning techniques are used to automatically translate text from one language to another.\n",
    "\n",
    "## b. Clear Problem Statement\n",
    "A large amount of digital content and documentation is available in the English language, which creates difficulties for users who primarily understand Hindi. Manual translation of English documents into Hindi is time-consuming and not always feasible.\n",
    "\n",
    "The problem addressed in this project is to build an automated system that can translate English text into Hindi accurately, using a machine learning model trained on parallel language data.\n",
    "\n",
    "## c. Real-World Relevance and Motivation\n",
    "English-to-Hindi translation is highly relevant in a multilingual country like India. Such a system can help users understand documents, educational material, instructions, and online content written in English.\n",
    "\n",
    "The motivation behind this project is to reduce the language barrier and demonstrate how deep learning can be used to solve real-world language problems. The project also provides practical experience in building and training a custom translation model using the PyTorch framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d618a26d-cfd4-4b88-b21d-398a6b3b0260",
   "metadata": {},
   "source": [
    "# Section 2: Data Understanding & Preparation\n",
    "\n",
    "## a. Dataset Source\n",
    "The dataset used in this project is a publicly available IIT Bombay English–Hindi parallel corpus, obtained from Kaggle. It consists of sentence pairs where each English sentence has a corresponding Hindi translation. This type of dataset is commonly used for training machine translation models and is suitable for supervised learning.\n",
    "\n",
    "## b. Data Loading and Exploration\n",
    "The dataset is loaded using the Pandas library. After loading, basic exploration is performed to understand the structure of the data, including the total number of sentence pairs and sample entries from the dataset.\n",
    "\n",
    "Initial exploration helps verify that the dataset contains aligned English and Hindi sentences and allows inspection of sentence length and content before preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43cac22b-ae3c-4ab8-a48b-b99f2be486d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 1561841\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hindi</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें</td>\n",
       "      <td>Give your application an accessibility workout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>एक्सेर्साइसर पहुंचनीयता अन्वेषक</td>\n",
       "      <td>Accerciser Accessibility Explorer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>निचले पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
       "      <td>The default plugin layout for the bottom panel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
       "      <td>The default plugin layout for the top panel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...</td>\n",
       "      <td>A list of plugins that are disabled by default</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               hindi  \\\n",
       "0    अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें   \n",
       "1                    एक्सेर्साइसर पहुंचनीयता अन्वेषक   \n",
       "2              निचले पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
       "3               ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
       "4  उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...   \n",
       "\n",
       "                                          english  \n",
       "0  Give your application an accessibility workout  \n",
       "1               Accerciser Accessibility Explorer  \n",
       "2  The default plugin layout for the bottom panel  \n",
       "3     The default plugin layout for the top panel  \n",
       "4  A list of plugins that are disabled by default  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"data/hindi_english_parallel.csv\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"Total rows:\", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3cfdea6-1c97-4297-ba96-e134bcd2aba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after sampling: 10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hindi</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>957248</th>\n",
       "      <td>बडे पैमाने पर सुनामी से प्रभावीत जापान में 4 द...</td>\n",
       "      <td>4 days after the massive tsunami struck Japan,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072034</th>\n",
       "      <td>वर्ग का पूर्णा क्या था?</td>\n",
       "      <td>What was completing the square?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195844</th>\n",
       "      <td>मैं अपना काम कर चुका हूँ।</td>\n",
       "      <td>I have already done my work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123517</th>\n",
       "      <td>राष्ट्रीय मनः स्वास्थ्य कार्यक्रम</td>\n",
       "      <td>National Mental Health Programme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933515</th>\n",
       "      <td>क्रियावली</td>\n",
       "      <td>menu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     hindi  \\\n",
       "957248   बडे पैमाने पर सुनामी से प्रभावीत जापान में 4 द...   \n",
       "1072034                           वर्ग का पूर्णा क्या था?    \n",
       "1195844                         मैं अपना काम कर चुका हूँ।    \n",
       "1123517                  राष्ट्रीय मनः स्वास्थ्य कार्यक्रम   \n",
       "933515                                           क्रियावली   \n",
       "\n",
       "                                                   english  \n",
       "957248   4 days after the massive tsunami struck Japan,...  \n",
       "1072034                    What was completing the square?  \n",
       "1195844                       I have already done my work.  \n",
       "1123517                   National Mental Health Programme  \n",
       "933515                                                menu  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle and sample the dataset\n",
    "df = df.sample(n=10000, random_state=42)\n",
    "\n",
    "print(\"Rows after sampling:\", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f014529d-1daf-450d-b3aa-3a3d6bd7e6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentence pairs: 10000\n",
      "('4 days after the massive tsunami struck Japan, hopes of finding anyone still alive were fading.', 'बडे पैमाने पर सुनामी से प्रभावीत जापान में 4 दिनो बाद कोई अभी तक जिंदा होने की आशाएँ लुप्त हो रही थी।')\n"
     ]
    }
   ],
   "source": [
    "sentence_pairs = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    english = str(row['english']).strip()\n",
    "    hindi = str(row['hindi']).strip()\n",
    "    \n",
    "    if english and hindi:\n",
    "        sentence_pairs.append((english, hindi))\n",
    "\n",
    "print(\"Total sentence pairs:\", len(sentence_pairs))\n",
    "print(sentence_pairs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514bcd3-71ff-414c-a133-319cf95c0c53",
   "metadata": {},
   "source": [
    "## c. Cleaning, Preprocessing, and Feature Engineering\n",
    "Several preprocessing steps are applied to prepare the data for model training:\n",
    "- All English text is converted to lowercase.\n",
    "- Unnecessary characters and punctuation are removed.\n",
    "- Hindi text is normalized using Unicode normalization to ensure consistency.\n",
    "- Extra spaces are removed from both English and Hindi sentences.\n",
    "- Special tokens such as <sos> (start of sentence) and <eos> (end of sentence) are added.\n",
    "- Vocabulary dictionaries are created for both English and Hindi languages by assigning unique indices to each word.\n",
    "Vocabulary dictionaries are created for both English and Hindi languages by assigning unique indices to each word.\n",
    "\n",
    "## d. Handling Missing Values or Noise\n",
    "The dataset does not contain missing values in the sentence pairs. However, noisy data such as extremely short or invalid sentences is filtered out during preprocessing. Sentences with insufficient length are removed to improve the quality of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d6126d6-d624-472d-95e0-eb9ab8d504c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: 7801\n",
      "('days after the massive tsunami struck japan, hopes of finding anyone still alive were fading.', 'बडे पैमाने पर सुनामी से प्रभावीत जापान में दिनो बाद कोई अभी तक जिंदा होने की आशाएँ लुप्त हो रही थी।')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_english(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z?.!,]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_hindi(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r\"[^\\u0900-\\u097F?.!,]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "cleaned_pairs = []\n",
    "\n",
    "for en, hi in sentence_pairs:\n",
    "    en_clean = clean_english(en)\n",
    "    hi_clean = clean_hindi(hi)\n",
    "    \n",
    "    if len(en_clean.split()) > 1 and len(hi_clean.split()) > 1:\n",
    "        cleaned_pairs.append((en_clean, hi_clean))\n",
    "\n",
    "print(\"After cleaning:\", len(cleaned_pairs))\n",
    "print(cleaned_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d712e97-0ae4-4807-bd1e-2dffdd0170fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: 7801\n",
      "After filtering: 6557\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 30\n",
    "\n",
    "filtered_pairs = []\n",
    "for en, hi in cleaned_pairs:\n",
    "    if len(en.split()) <= MAX_LEN and len(hi.split()) <= MAX_LEN:\n",
    "        filtered_pairs.append((en, hi))\n",
    "\n",
    "print(\"Before filtering:\", len(cleaned_pairs))\n",
    "print(\"After filtering:\", len(filtered_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6415a266-fe17-42f8-9ba6-bc41d7fc9e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs used for training: 4500\n"
     ]
    }
   ],
   "source": [
    "# Reduce dataset size for training\n",
    "filtered_pairs = filtered_pairs[:4500]\n",
    "\n",
    "print(\"Pairs used for training:\", len(filtered_pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1194344-7d1e-4a12-b9dd-5b4623a651d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3500\n",
      "Test: 1000\n"
     ]
    }
   ],
   "source": [
    "train_pairs = filtered_pairs[:3500]\n",
    "test_pairs = filtered_pairs[3500:4500]\n",
    "\n",
    "print(\"Train:\", len(train_pairs))\n",
    "print(\"Test:\", len(test_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b20480bc-fc4f-4241-82cc-2fa0299d74ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "SOS_TOKEN = \"<sos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "UNK_TOKEN = \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fde8c24-7bad-4b26-b13e-b8d6a7227232",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {\n",
    "            PAD_TOKEN: 0,\n",
    "            SOS_TOKEN: 1,\n",
    "            EOS_TOKEN: 2,\n",
    "            UNK_TOKEN: 3\n",
    "        }\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        \n",
    "        # FIX: initialize word_count with special tokens\n",
    "        self.word_count = {\n",
    "            PAD_TOKEN: 0,\n",
    "            SOS_TOKEN: 0,\n",
    "            EOS_TOKEN: 0,\n",
    "            UNK_TOKEN: 0\n",
    "        }\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split():\n",
    "            if word not in self.word2idx:\n",
    "                idx = len(self.word2idx)\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                self.word_count[word] = 1\n",
    "            else:\n",
    "                self.word_count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fc67481-dc84-41b9-939b-eb307887e6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<sos> days after the massive tsunami struck japan, hopes of finding anyone still alive were fading. <eos>', '<sos> बडे पैमाने पर सुनामी से प्रभावीत जापान में दिनो बाद कोई अभी तक जिंदा होने की आशाएँ लुप्त हो रही थी। <eos>')\n"
     ]
    }
   ],
   "source": [
    "tokenized_pairs = []\n",
    "\n",
    "for en, hi in train_pairs:\n",
    "    en_sentence = f\"{SOS_TOKEN} {en} {EOS_TOKEN}\"\n",
    "    hi_sentence = f\"{SOS_TOKEN} {hi} {EOS_TOKEN}\"\n",
    "    tokenized_pairs.append((en_sentence, hi_sentence))\n",
    "\n",
    "print(tokenized_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99a6ec2b-18a7-4eb5-8d22-133eb4b125df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size: 9718\n",
      "Hindi vocab size: 9679\n"
     ]
    }
   ],
   "source": [
    "en_vocab = Vocabulary()\n",
    "hi_vocab = Vocabulary()\n",
    "\n",
    "for en_sentence, hi_sentence in tokenized_pairs:\n",
    "    en_vocab.add_sentence(en_sentence)\n",
    "    hi_vocab.add_sentence(hi_sentence)\n",
    "\n",
    "print(\"English vocab size:\", len(en_vocab.word2idx))\n",
    "print(\"Hindi vocab size:\", len(hi_vocab.word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3197cb68-b589-4094-862a-269990e71f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 2], [1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2])\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [\n",
    "        vocab.word2idx.get(word, vocab.word2idx[UNK_TOKEN])\n",
    "        for word in sentence.split()\n",
    "    ]\n",
    "\n",
    "numeric_pairs = [\n",
    "    (sentence_to_indices(en, en_vocab),\n",
    "     sentence_to_indices(hi, hi_vocab))\n",
    "    for en, hi in tokenized_pairs\n",
    "]\n",
    "\n",
    "print(numeric_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d19f2c85-5629-4eff-a734-a833fb761bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 30\n",
    "\n",
    "def pad_sequence(seq, max_len, pad_idx=0):\n",
    "    if len(seq) < max_len:\n",
    "        return seq + [pad_idx] * (max_len - len(seq))\n",
    "    else:\n",
    "        return seq[:max_len]\n",
    "\n",
    "padded_pairs = [\n",
    "    (pad_sequence(en, MAX_LEN),\n",
    "     pad_sequence(hi, MAX_LEN))\n",
    "    for en, hi in numeric_pairs\n",
    "]\n",
    "\n",
    "print(padded_pairs[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05684d72-c47e-400f-b759-e14afaff5738",
   "metadata": {},
   "source": [
    "# Section 3: Model / System Design\n",
    "\n",
    "## a. AI Technique Used\n",
    "This project uses Deep Learning–based Natural Language Processing (NLP) techniques. Specifically, a Neural Machine Translation (NMT) approach is implemented using a Sequence-to-Sequence (Seq2Seq) model with an Attention mechanism.\n",
    "\n",
    "The model is trained from scratch using the PyTorch deep learning framework. No pre-trained language models or large language models (LLMs) are used in this project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd854a9-df5e-4ec4-8893-71e5366adbce",
   "metadata": {},
   "source": [
    "## b. Model Architecture\n",
    "\n",
    "### Encoder\n",
    "\n",
    "- The encoder is implemented using an LSTM network.\n",
    "- It takes tokenized English sentences as input.\n",
    "- Each word is converted into an embedding vector.\n",
    "- The encoder outputs hidden states that capture the meaning of the input sentence.\n",
    "\n",
    "### Decoder\n",
    "\n",
    "- The decoder is also implemented using an LSTM network.\n",
    "- It generates the Hindi translation word by word.\n",
    "- At each time step, the decoder uses attention to focus on relevant encoder states.\n",
    "- The output is a probability distribution over the Hindi vocabulary.\n",
    "\n",
    "### Attention Mechanism\n",
    "\n",
    "- The attention mechanism allows the model to align input and output words.\n",
    "- Instead of relying on a single fixed context vector, attention dynamically weighs encoder outputs.\n",
    "- This improves translation quality, especially for longer sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165204ad-59c7-4e91-81ae-faf209b7e9b6",
   "metadata": {},
   "source": [
    "## c. Justification of Design Choices \n",
    "\n",
    "A Seq2Seq model with attention was chosen because it is well-suited for language translation tasks where input and output sequences have different lengths. The attention mechanism improves translation quality by helping the decoder focus on important words in the input sentence.\n",
    "\n",
    "LSTM networks were selected due to their ability to handle sequential data and capture contextual dependencies in language. PyTorch was chosen as the implementation framework because of its flexibility, ease of debugging, and strong support for deep learning research.\n",
    "\n",
    "To ensure feasible training on a CPU-based system, sentence length and dataset size were constrained. These design choices allowed successful model training while still demonstrating the complete working of a neural machine translation system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aca8ab-fd0f-4f51-902b-d0a05557bfab",
   "metadata": {},
   "source": [
    "# Section 4: Core Implementation\n",
    "\n",
    "This section covers the implementation of the Neural Machine Translation system, including\n",
    "the encoder, decoder, attention mechanism, and training pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c9afc9a8-c861-4cf1-b561-96447f39e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d91ce9d5-8c6a-4017-8c48-bda40c7b4500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "317c4448-dd28-43fc-9fd7-8a2e80ecabd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(en_vocab.word2idx)    # English vocabulary size\n",
    "OUTPUT_DIM = len(hi_vocab.word2idx)   # Hindi vocabulary size\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f2911b4-0b30-42cf-8f1b-5d0df6aaa54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: (batch_size, seq_len)\n",
    "        embedded = self.embedding(src)\n",
    "        # embedded shape: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        return outputs, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "126d47db-eab9-4f51-a125-2b6d6469e92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden shape: (num_layers, batch_size, hidden_dim)\n",
    "        # encoder_outputs shape: (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "\n",
    "        # Use the last layer's hidden state\n",
    "        hidden = hidden[-1].unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        # hidden shape: (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # energy shape: (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        # attention shape: (batch_size, seq_len)\n",
    "\n",
    "        return torch.softmax(attention, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e907a3c0-18e5-4404-9965-082e8216a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, num_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim + hidden_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        # input shape: (batch_size)\n",
    "        input = input.unsqueeze(1)\n",
    "        # input shape: (batch_size, 1)\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded shape: (batch_size, 1, embedding_dim)\n",
    "\n",
    "        attention_weights = self.attention(hidden, encoder_outputs)\n",
    "        # attention_weights shape: (batch_size, seq_len)\n",
    "\n",
    "        attention_weights = attention_weights.unsqueeze(1)\n",
    "        # shape: (batch_size, 1, seq_len)\n",
    "\n",
    "        context = torch.bmm(attention_weights, encoder_outputs)\n",
    "        # context shape: (batch_size, 1, hidden_dim)\n",
    "\n",
    "        lstm_input = torch.cat((embedded, context), dim=2)\n",
    "        # shape: (batch_size, 1, embedding_dim + hidden_dim)\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "        # output shape: (batch_size, 1, hidden_dim)\n",
    "\n",
    "        output = output.squeeze(1)\n",
    "        context = context.squeeze(1)\n",
    "\n",
    "        prediction = self.fc_out(torch.cat((output, context), dim=1))\n",
    "        # prediction shape: (batch_size, output_dim)\n",
    "\n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f2caf8-e5c2-48dd-893c-57541274c349",
   "metadata": {},
   "source": [
    "## a. Model Training\n",
    "\n",
    "The model is trained using a Seq2Seq architecture with an attention mechanism on English–Hindi sentence pairs. Training is performed using the Adam optimizer and Cross-Entropy loss. After training, the model is switched to evaluation mode to generate Hindi translations for unseen English sentences.\n",
    "\n",
    "## b. Prompt Engineering\n",
    "\n",
    "Prompt engineering is **not applicable** to this project, as the system does not use large language models (LLMs) or prompt-based inference. Instead, the model is trained from scratch using supervised learning on a parallel corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c018eff5-60d3-4448-88d3-73f21521ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src shape: (batch_size, src_len)\n",
    "        # trg shape: (batch_size, trg_len)\n",
    "\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "\n",
    "        input = trg[:, 0]  # <sos>\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(\n",
    "                input, hidden, cell, encoder_outputs\n",
    "            )\n",
    "\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "405e2f91-705e-4dc7-982c-bfb4bd9ca3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, loss, and optimizer initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize attention\n",
    "attention = Attention(HIDDEN_DIM)\n",
    "\n",
    "# Initialize encoder\n",
    "encoder = Encoder(\n",
    "    input_dim=INPUT_DIM,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "# Initialize decoder\n",
    "decoder = Decoder(\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    attention=attention\n",
    ")\n",
    "\n",
    "# Initialize Seq2Seq model\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Define loss function (ignore padding)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Model, loss, and optimizer initialized successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65986b77-14fd-4f15-a117-16f2e922a2a0",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba3869ae-2f39-451a-9f2d-5946d5456971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab.word2idx.get(word, vocab.word2idx[\"<unk>\"]) for word in sentence.split()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc442033-7222-40c4-aa91-f1c53badc15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_pairs = []\n",
    "\n",
    "for en, hi in tokenized_pairs:\n",
    "    en_ids = sentence_to_indices(en, en_vocab)\n",
    "    hi_ids = sentence_to_indices(hi, hi_vocab)\n",
    "    indexed_pairs.append((en_ids, hi_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d2165bb-33f5-4fd1-a2dd-febd70723015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dataset size for training due to computational constraints\n",
    "indexed_pairs = indexed_pairs[:3000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "183d9a12-7a4e-4bba-ba4e-6dc6de6ca982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_pairs(pairs):\n",
    "    src_seqs = [torch.tensor(pair[0]) for pair in pairs]\n",
    "    trg_seqs = [torch.tensor(pair[1]) for pair in pairs]\n",
    "\n",
    "    src_padded = pad_sequence(src_seqs, batch_first=True, padding_value=0)\n",
    "    trg_padded = pad_sequence(trg_seqs, batch_first=True, padding_value=0)\n",
    "\n",
    "    return src_padded, trg_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd019950-6902-48cc-a10b-24c6741cb3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader created successfully\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "src_data, trg_data = pad_pairs(indexed_pairs)\n",
    "\n",
    "dataset = TensorDataset(src_data, trg_data)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"DataLoader created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f5a582-7459-4406-8024-59e85e4e22e3",
   "metadata": {},
   "source": [
    "## c. Translation Pipeline\n",
    "\n",
    "The translation pipeline follows a structured sequence of steps:\n",
    "- Input English text is tokenized and converted into numerical indices.\n",
    "- The encoder processes the input sequence to generate contextual representations.\n",
    "- The attention mechanism computes alignment between input and output tokens.\n",
    "- The decoder predicts the next Hindi word at each time step.\n",
    "- Translation continues until an end-of-sentence token is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "89fcf298-5814-4775-91d5-32efdf4539c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src, trg in dataloader:\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "        # output shape: (batch_size, trg_len, OUTPUT_DIM)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f07fb1cd-ddd8-44f7-86c0-63b9057be824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 7.4306\n",
      "Epoch 2/20, Loss: 6.7865\n",
      "Epoch 3/20, Loss: 6.4642\n",
      "Epoch 4/20, Loss: 6.0303\n",
      "Epoch 5/20, Loss: 5.3755\n",
      "Epoch 6/20, Loss: 4.5387\n",
      "Epoch 7/20, Loss: 3.7104\n",
      "Epoch 8/20, Loss: 3.1050\n",
      "Epoch 9/20, Loss: 2.7798\n",
      "Epoch 10/20, Loss: 2.5133\n",
      "Epoch 11/20, Loss: 2.2736\n",
      "Epoch 12/20, Loss: 2.0455\n",
      "Epoch 13/20, Loss: 1.8587\n",
      "Epoch 14/20, Loss: 1.6609\n",
      "Epoch 15/20, Loss: 1.4923\n",
      "Epoch 16/20, Loss: 1.3426\n",
      "Epoch 17/20, Loss: 1.2064\n",
      "Epoch 18/20, Loss: 1.1034\n",
      "Epoch 19/20, Loss: 0.9698\n",
      "Epoch 20/20, Loss: 0.8828\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss = train_model(model, dataloader, optimizer, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649221ab-759e-49c9-8f50-b451b15375f6",
   "metadata": {},
   "source": [
    "# 5. Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d1cbd-8d6b-42ac-9f7f-6426696538a8",
   "metadata": {},
   "source": [
    "## a. Metrics Used\n",
    "The model is evaluated using Cross-Entropy Loss during training, which measures how accurately the predicted Hindi words match the target Hindi words. A continuous decrease in loss across epochs indicates that the model is learning meaningful language patterns.\n",
    "\n",
    "Along with loss, a qualitative evaluation is performed by testing the trained model on a few sample English sentences and manually observing the translated Hindi output. This helps understand how well the model performs in real translation scenarios, especially for sentence structure and word selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "63eed5b7-775b-488c-8f5e-4f36c92a1254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model set to evaluation mode\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(\"Model set to evaluation mode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "37ba5487-0042-4614-a0c4-5c4c306f691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, en_vocab, hi_vocab, device, max_len=30):\n",
    "    model.eval()\n",
    "\n",
    "    # tokenize source sentence\n",
    "    tokens = sentence.lower().split()\n",
    "    tokens = [\"<sos>\"] + tokens + [\"<eos>\"]\n",
    "\n",
    "    src_indices = [\n",
    "        en_vocab.word2idx.get(token, en_vocab.word2idx[\"<unk>\"])\n",
    "        for token in tokens\n",
    "    ]\n",
    "\n",
    "    src_tensor = torch.tensor(src_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden, cell = model.encoder(src_tensor)\n",
    "\n",
    "    # START target sequence\n",
    "    trg_indices = [hi_vocab.word2idx[\"<sos>\"]]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.tensor([trg_indices[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(\n",
    "                trg_tensor, hidden, cell, encoder_outputs\n",
    "            )\n",
    "\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indices.append(pred_token)\n",
    "\n",
    "        if pred_token == hi_vocab.word2idx[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    # convert indices to words (remove <sos>, stop at <eos>)\n",
    "    trg_tokens = []\n",
    "    for idx in trg_indices:\n",
    "        token = hi_vocab.idx2word.get(idx, \"<unk>\")\n",
    "        if token == \"<eos>\":\n",
    "            break\n",
    "        trg_tokens.append(token)\n",
    "\n",
    "    return trg_tokens[1:]  # remove <sos>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f344f23-1db7-45d7-8782-7de7e2cdf582",
   "metadata": {},
   "source": [
    "## b. Sample Outputs\n",
    "After training, the model is tested on unseen English sentences. The model generates Hindi translations word-by-word using the trained Seq2Seq + Attention architecture.\n",
    "\n",
    "The observed results show that the model is able to produce Hindi tokens and short phrases, and in some cases it generates meaningful Hindi words. However, the translations are often not fully accurate, and the output may contain repeated words, incomplete sentence meaning, or incorrect grammar.\n",
    "\n",
    "This demonstrates that the model has learned basic word associations and sentence patterns, but the translation quality is still limited compared to a real-world production translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "425edaf4-0a42-4c1f-9f07-41fb0761fbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: i am a student\n",
      "Hindi: मैं हूँ।\n",
      "----------------------------------------\n",
      "English: he is a boy\n",
      "Hindi: है।\n",
      "----------------------------------------\n",
      "English: she is my friend\n",
      "Hindi: है. है।\n",
      "----------------------------------------\n",
      "English: she like studying maths\n",
      "Hindi: जैसे ली।\n",
      "----------------------------------------\n",
      "English: we are learning\n",
      "Hindi: हम चाहते हैं\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"i am a student\",\n",
    "    \"he is a boy\",\n",
    "    \"she is my friend\",\n",
    "    \"she like studying maths\",\n",
    "    \"we are learning\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    translation = translate_sentence(sentence, model, en_vocab, hi_vocab, device, max_len=30)\n",
    "    translation = [w for w in translation if w not in [\"<sos>\", \"<eos>\", \"<pad>\"]]\n",
    "\n",
    "    print(f\"English: {sentence}\")\n",
    "    print(f\"Hindi: {' '.join(translation)}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63c636-ed0e-415d-bbe5-f79bc9f21f6e",
   "metadata": {},
   "source": [
    "## Text File Translation (TXT Input)\n",
    "\n",
    "This part of the project allows translating a plain text (.txt) file from English to Hindi.\n",
    "The file is read, its content is extracted as text, and the trained model translates it line-by-line or sentence-by-sentence.\n",
    "The translated output is then saved into a new text file for easy viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "29691d37-de2a-4035-99ed-5fabd0bb3b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def translate_text_block(text, model, en_vocab, hi_vocab, device, max_len=30):\n",
    "    \"\"\"\n",
    "    Translates a block of English text into Hindi using the trained model.\n",
    "    Works line-by-line to keep it simple and stable.\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    translated_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            translated_lines.append(\"\")\n",
    "            continue\n",
    "\n",
    "        # basic cleanup to make model behave slightly better\n",
    "        line_clean = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", line).lower().strip()\n",
    "\n",
    "        # very short/empty lines skip\n",
    "        if len(line_clean.split()) == 0:\n",
    "            translated_lines.append(\"\")\n",
    "            continue\n",
    "\n",
    "        translated_tokens = translate_sentence(line_clean, model, en_vocab, hi_vocab, device, max_len=max_len)\n",
    "        translated_lines.append(\" \".join(translated_tokens))\n",
    "\n",
    "    return \"\\n\".join(translated_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d95dab31-d21d-4982-a855-c04b6d6636ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_txt_file(input_path, output_path, model, en_vocab, hi_vocab, device, max_len=30):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    translated_text = translate_text_block(text, model, en_vocab, hi_vocab, device, max_len=max_len)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(translated_text)\n",
    "\n",
    "    print(\"TXT Translation Completed ✅\")\n",
    "    print(\"Saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fa5e41fb-eba1-41ac-bd00-0336bede4518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_txt_file(input_path, output_path, model, en_vocab, hi_vocab, device, max_len=30):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    translated_text = translate_text_block(text, model, en_vocab, hi_vocab, device, max_len=max_len)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(translated_text)\n",
    "\n",
    "    print(\"TXT Translation Completed ✅\")\n",
    "    print(\"Saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "77242fcd-fa89-45c0-bb0b-b97ab773028e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: sample_input.txt\n"
     ]
    }
   ],
   "source": [
    "sample_txt_path = \"sample_input.txt\"\n",
    "with open(sample_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"I am going to school.\\nHe is my friend.\\nThis is a good day.\")\n",
    "print(\"Created:\", sample_txt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2f43febb-2fda-427d-940d-6080eea21586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TXT Translation Completed ✅\n",
      "Saved to: translated_output.txt\n"
     ]
    }
   ],
   "source": [
    "translate_txt_file(\n",
    "    input_path=\"sample_input.txt\",\n",
    "    output_path=\"translated_output.txt\",\n",
    "    model=model,\n",
    "    en_vocab=en_vocab,\n",
    "    hi_vocab=hi_vocab,\n",
    "    device=device,\n",
    "    max_len=30\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "33e05639-9c8a-4e01-b899-946226922478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "मुझे संदेश रहा है।\n",
      "है।\n",
      "यह भी होता है।\n"
     ]
    }
   ],
   "source": [
    "with open(\"translated_output.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3337b2-a4e9-4509-ad56-40a729e16828",
   "metadata": {},
   "source": [
    "## PDF Translation (PDF Input)\n",
    "\n",
    "This section adds support for translating PDF documents.\n",
    "Using the PyMuPDF (fitz) library, the text content is extracted from the PDF (limited pages can be chosen).\n",
    "The extracted English text is then passed to the translation model, and the final Hindi translation is saved into an output .txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6258c847-872b-4c1e-8b0d-22be4416882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, max_pages=5):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_parts = []\n",
    "    pages_to_read = min(len(doc), max_pages)\n",
    "\n",
    "    for i in range(pages_to_read):\n",
    "        page = doc[i]\n",
    "        text_parts.append(page.get_text())\n",
    "\n",
    "    doc.close()\n",
    "    return \"\\n\".join(text_parts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "77070978-0e12-407a-b2b2-ab65b1eadb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_pdf_file(pdf_path, output_txt_path, model, en_vocab, hi_vocab, device, max_len=30, max_pages=5):\n",
    "    \n",
    "    extracted_text = extract_text_from_pdf(pdf_path, max_pages=max_pages)\n",
    "\n",
    "    translated_text = translate_text_block(extracted_text, model, en_vocab, hi_vocab, device, max_len=max_len)\n",
    "\n",
    "    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(translated_text)\n",
    "\n",
    "    print(\"PDF Translation Completed ✅\")\n",
    "    print(\"Saved to:\", output_txt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "493051f8-6eae-4348-bfb2-f4c856fdc530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Translation Completed ✅\n",
      "Saved to: translated_pdf_output.txt\n"
     ]
    }
   ],
   "source": [
    "translate_pdf_file(\n",
    "    pdf_path=\"english.pdf\",\n",
    "    output_txt_path=\"translated_pdf_output.txt\",\n",
    "    model=model,\n",
    "    en_vocab=en_vocab,\n",
    "    hi_vocab=hi_vocab,\n",
    "    device=device,\n",
    "    max_len=30,\n",
    "    max_pages=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5d0e29-173f-493b-bb14-077da84a9b2e",
   "metadata": {},
   "source": [
    "## c. Performance Analysis and Limitations\n",
    "\n",
    "Training loss reduces significantly with increasing epochs, which confirms that the model learns from the dataset and improves internally. However, the final translation quality is affected by several limitations:\n",
    "- The model is trained from scratch without using large pre-trained transformer models.\n",
    "- The dataset size is reduced for faster training, which limits language exposure.\n",
    "- The system is trained on CPU, so training complexity and epochs are restricted.\n",
    "- The translations may fail on long or complex sentences due to limited context understanding.\n",
    "- The output sometimes contains repetition or incorrect word ordering, showing that the model still struggles with fluency.\n",
    "Overall, the project successfully demonstrates a working academic prototype of an English-to-Hindi translation system using deep learning, but further improvements are required for high-quality translation output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a612c-7bb3-4205-8a6f-d5020ff65cbc",
   "metadata": {},
   "source": [
    "# 6. Ethical Considerations & Responsible AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354642e8-2d0c-4853-a11f-3ab1fd85a5e3",
   "metadata": {},
   "source": [
    "## a. Bias and Fairness Considerations\n",
    "\n",
    "- The translation model may reflect biases present in the training dataset, such as gender stereotypes or incorrect assumptions in certain sentences.\n",
    "- Some words or phrases may be translated inaccurately depending on context, which can lead to unfair or misleading outputs.\n",
    "- The model may perform better on common sentence patterns but worse on less frequent topics, which creates unequal performance across different user inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf16fb-62b0-48a9-bf62-b0c725e04880",
   "metadata": {},
   "source": [
    "## b. Dataset Limitations\n",
    "\n",
    "- The dataset contains limited vocabulary coverage and does not represent all types of English and Hindi language usage.\n",
    "- Some sentence pairs may contain noise, informal text, or inconsistent translations, which impacts model learning.\n",
    "- The dataset may not include domain-specific language (medical, legal, technical), so translations in such contexts may be incorrect.\n",
    "- Since a reduced subset of the dataset was used for training due to computational constraints, the model may not generalize well to unseen sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24612d18-246e-4e70-873b-3017a8916ece",
   "metadata": {},
   "source": [
    "## c. Responsible Use of AI Tools\n",
    "\n",
    "- The translation system should be used as an academic prototype and not as a final trusted translator.\n",
    "- The model output should be verified by humans before using it in important contexts such as healthcare, legal documents, or official communication.\n",
    "- Users should be aware that the model can produce incorrect or misleading translations and should not blindly rely on it.\n",
    "- Responsible development includes clearly mentioning limitations and avoiding false claims about accuracy or performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ea031-eb1b-4791-a8b9-a771914757a2",
   "metadata": {},
   "source": [
    "# 7. Conclusion and Future Scope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747bf5e0-695c-4daa-b2dc-25ef4c3c3a32",
   "metadata": {},
   "source": [
    "## a. Summary of Results\n",
    "\n",
    "- An English-to-Hindi Neural Machine Translation system was successfully implemented using a Seq2Seq model with Attention in PyTorch.\n",
    "- The complete workflow was achieved, including dataset loading, preprocessing, vocabulary creation, model training, inference, and result analysis.\n",
    "- Training loss decreased consistently across epochs, indicating that the model learned useful translation patterns from the dataset.\n",
    "- The system supports translation of single input sentences and also provides utilities for translating TXT files and PDF documents by extracting and processing the text.\n",
    "- Although the translations are not always fully accurate or fluent, the project demonstrates a complete end-to-end translation pipeline and validates the practical implementation of a neural machine translation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a4dd7b-94f6-41ed-98ef-80a59912a3c2",
   "metadata": {},
   "source": [
    "## b. Possible Improvements and Extensions\n",
    "\n",
    "- Train the model on a larger and more diverse English–Hindi dataset to improve translation accuracy and vocabulary coverage.\n",
    "- Use a proper train/validation/test split and apply evaluation metrics more consistently for reliable performance measurement.\n",
    "- Replace the Seq2Seq model with a Transformer-based architecture to achieve better long-range context understanding and more fluent translations.\n",
    "- Improve decoding by using beam search instead of greedy decoding to reduce repetition and generate more meaningful output sentences.\n",
    "- Optimize training using GPU acceleration, which will allow larger models, higher epochs, and faster experimentation.\n",
    "- Enhance file translation by improving PDF text extraction for scanned or complex PDFs and adding formatting-aware translation for better readability.\n",
    "- Develop a simple GUI or web application where users can upload TXT/PDF files and download translated output easily.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
